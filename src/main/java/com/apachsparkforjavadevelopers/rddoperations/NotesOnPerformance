Shuffles:
A stage is a series of narrow transformations. If a shuffle is required, Spark will create a new stage.

A shuffle occurs prior to a wide transformation (i.e. a transformation that requires data to be exchanged between
partitions). Shuffles occur at the end end of a stage. Data is written to disk and serialized before being send across
the network.

Stages can be skipped when an action would require the stage to be rerun but the shuffle write data is available on disk.
From there the narrowly transformed data of a stage can be read again by the subsequent stage. The data written to disk
is compressed.

The subsequent stage will read the shuffle data back in (shuffle read).

Caching and persistence:
While stages can be skipped if shuffle data from the previous stage is available on disk, all narrow transformations
will have to be executed against the data once again. Caching the data after the last transformation can avoid having to
 rerun the transformations.

 Caching only works when there's enough space in memory. If there's not enough space, Spark will continue the execution
 without caching the data in memory.k